# Homework 1: Benchmarking the Memory Hierarchy
### Eli Poppele
All my code for this project is in the "benchmark.cc" file. It contains a loop that calls the timing function for each buffer size several times, and records the minimum time for memory access for that buffer size. The times print to the command line, and were then copied into "times.txt" (with minimums from 50 iterations) and used to create the time.png graph in the repository.

## Iteration Scheme
To time the memory access, I attempted to create a scheme for accessing and iterating over my buffer that the prefetcher would not be able to predict, thus forcing it to prefetch as much of the buffer as possible. For the buffer, I used an array of bytes in the form of unsigned 8-bit integers (uint8_t). The array was then made two-dimensional, divided into subarrays of length `div_num`. Some experimentation suggested that 256 as the best option for this. The values of the array were then randomly generated between 0 and 255, the largest value that the byte could hold. In the timing loop, I then ran 256 timing loops on the buffer. In each loop, I created a new random permutation of `sub_count`, the number of subarrays that the buffer held, as another array. I then started the timing, and looped 1024 times over the memory access. I found that running the loop a constant number of times helped to make smaller buffers more consistent in timing.
Within the timing loop, I read the byte with `c = buffer[permutation[j%sub_count]][c]`, where `j` was the loop variable increasing by 1. `c` started at 0, but was overwritten with each memory read. Since the permutation was random, I would be accessing a random subarray with each read. With `c` starting at 0, the first read was somewhat deterministic, but after that, the index within the random subarray would be totally random, since each subarray held 256 bytes, each with a value from 0 to 255, so they could be used to find a new random index to access in the next subarray. 
In this manner, I managed to access relatively random bytes from the array, without timing siginificantly more than the memory access. The modulo could be a relatively time expensive function, but since `j` is always in the range of 0 to 1023, the time for that operation doesn't really scale with buffer size. (I figure at worst there is an inverse relation; `j%4` taking longer than `j%512`.) Accessing `c` is also pretty constant, and although accessing `permutation` at some index is going to scale, it is relatively small compared to my buffer; so assuming that permutation is in the L1 cache, it is not taking much space up at lower buffer sizes, when the buffer could fit in the cache, and it is not slowing much down at larger buffer sizes, since it is all in the cache (`permutation` never being larger than 256kb, which is the size of my L1 cache).

## Development
I used permutations after attempting to make an access pattern that used two permutation arrays to iterate over the array and access every byte in the array in a completely random order. This was not entirely successful, in part due to problems in other parts of my code. Changing to accessing from `c` in the second index was a somewhat random attempt to improve the code that seemed to work, after which I tweaked some values such as `div_num` until things seemed to run as best as they could. Earlier attempts included the use of prime numbers and modulo applied to the indexing, or attempting to "pollute the cache" by runnning heavy computations on the entire buffer that should try to load the whole thing into the cache. I also attempted to use random indices, but without the use of a pre-allocated array of random numbers, I could only generate one index outside of the loop, and so I used something like `c = buffer[j][random_i]`, with `j` being the timing loop variable and `i` generated outside of it. Using an array of random indices generated beforehand for `j` may have worked, similar to the permutation that I used in my final attempt; however, I think the permutation is slightly better, as it is also random access, but ensures that no subarray is accessed twice in one loop over the random index array; this guarantees that every part of the buffer is used as much as possible (such as for the larger buffers, with more than 1024 subarrays, where the permutation guarantees that as many of those subarrays as possible are accessed, while the strictly random iteration could refer to the same thing more than once). I'm not sure if this at all makes a difference to the prefetcher, but it at least made some sense to me.

## Results
My results appear in the graph below, and seem to support the ability of my scheme to foil the prefetcher.
![Timing Graph](/time.png)

My L1 cache can hold 256KB, and the graph seems constant with a low read time until that point is exceeded, where there is a more gentle climb to 1MB, which is the capacity of my L2 cache (although a buffer of 256KB, at maximum capacity, does show slower times than expected). Times begin to increase more rapidly past that point, and there is not a clear transition between L3 and DRAM, which should occur just after the L3 is full, at 6MB. This could suggest that there is less difference in the access time for my CPU between those two, as 63ns at 4MB, all in L3 cache, is not as far from 89ns at 16MB, being 10/16ths in DRAM. Larger buffers, at 2^27 and 2^28 bytes, seemed to platuea at times not much greater than the access time for 2^26 (64MB), which does show expected behavior--a continous linear increase past that point would suggest that something was off in my timing loop. On the other hand, these sizes were too low for me to run at many iterations, and since they were only run at 5 iterations instead of 50, I did not include them with the rest of my data on my graph. 
As stated above, the changes and scaling in timing seemed to be as expected; however, I should point out that everything was several times slower than expected. This was likely due to the other calculations included in the timing loop, as L1 access was 2.5ns slower, or 6 times as slow as expected, while the L2 access was closer to 7ns slower, or 2 times slower, and DRAM took only about 1.5 times longer than expected. 
Although further tests (and more time) would be needed to confirm these results, I believe the near step-like nature of timing results, as well as the plateauing of the extra large buffers, suggest that my code did actually function to load as much of the buffer as possible into the cache and time the access from different parts of the memory hierarchy. 

## Testing
I did not write any tests for this code, since all the testable functions were quite trivial, with the exception of the one thing I was really trying to do (foil the prefetcher) for which this entire project is essentially a test. 
Running valgrind on my code, I did not find any memory leaks.
